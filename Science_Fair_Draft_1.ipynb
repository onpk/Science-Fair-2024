{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1BERa8BoWuWh_TGO4VZcwLyO9JVB9Z8m3",
      "authorship_tag": "ABX9TyOp8V0oguYzIBfyR2WBvkrN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/onpk/Science-Fair-2024/blob/main/Science_Fair_Draft_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predicting the prescense of e-coli given data, using Computer Vision and CNNs\n",
        "\n",
        "Use Image Segmentation to isolate the bacteria in images where there are other types of bacteria\n",
        "\n",
        "If possible, use transfer learning to predict if a sample does not have bacteria in it"
      ],
      "metadata": {
        "id": "LRUGgGdTCr4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step one: Data Preprocessing and cleaning\n",
        "#Import necessary modules:\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "import os\n",
        "#from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "uYP0eTadoRQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Open Data(Have seperate folders for e-coli yes and no)\n",
        "from PIL import Image,ImageEnhance #Just to test if the images can open\n",
        "import random\n",
        "directory=\"/content/drive/My Drive/NR_Science_Fair_2024/Positive\"\n",
        "#drive.mount(\"/content/drive\")\n",
        "os.chdir(directory)\n",
        "poslist={}\n",
        "neglist={}\n",
        "direct=os.listdir(\"/content/drive/My Drive/NR_Science_Fair_2024/Positive\")\n",
        "for d in direct:\n",
        "  #temp=[]\n",
        "  img=Image.open(d)\n",
        "  img=img.convert(\"L\")\n",
        "  temp.append(np.asarray(img))\n",
        "  temp.append(\"Positive\")\n",
        "  poslist.append(temp)\n",
        "os.chdir(\"/content/drive/My Drive/NR_Science_Fair_2024/Negative\")\n",
        "direct=os.listdir(\"/content/drive/My Drive/NR_Science_Fair_2024/Negative\")\n",
        "for e in direct:\n",
        "  temp=[]\n",
        "  img=Image.open(e)\n",
        "  img=img.convert(\"L\")\n",
        "  temp.append(np.asarray(img))\n",
        "  temp.append(\"Negative\")\n",
        "  neglist.append(temp)\n",
        "#img=plt.imread(direct[0])\n",
        "data=poslist+neglist\n",
        "random.shuffle(data)\n",
        "#data=np.asarray(data)\n",
        "print(data)"
      ],
      "metadata": {
        "id": "ONzXeUsGqXr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting training and Testing Data (loop through the data and randomly pick 80% of both datasets to put into the training, and put the rest into testing)\n",
        "nparray=np.array(data,dtype=object)\n",
        "t_temp,v_temp=np.split(nparray,[int(len(nparray)*0.8)])\n",
        "t=[]\n",
        "v=[]\n",
        "t_labels=[]\n",
        "v_labels=[]\n",
        "for i in t_temp:\n",
        "  t.append(i[0])\n",
        "  t_labels.append(i[1])\n",
        "for j in v_temp:\n",
        "  v.append(j[0])\n",
        "  v_labels.append(j[1])\n",
        "t=np.array(t,dtype=object)\n",
        "v=np.array(v,dtype=object)\n",
        "print(t[1])\n",
        "'''print(len(data))\n",
        "flat_list = [item for sublist in data for item in sublist]\n",
        "flat_array = np.array(flat_list)\n",
        "\n",
        "# Calculate the index for splitting the array in an 80:20 ratio\n",
        "split_index = int(0.8 * len(flat_array))\n",
        "\n",
        "# Split the flat array into two arrays based on the 80:20 ratio\n",
        "t = flat_array[:split_index]\n",
        "v = flat_array[split_index:]\n",
        "print(v[1])'''"
      ],
      "metadata": {
        "id": "04bEFyjKwGvP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "b3026d84-8e70-4724-9630-c41b1ecc0595"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'print(len(data))\\nflat_list = [item for sublist in data for item in sublist]\\nflat_array = np.array(flat_list)\\n\\n# Calculate the index for splitting the array in an 80:20 ratio\\nsplit_index = int(0.8 * len(flat_array))\\n\\n# Split the flat array into two arrays based on the 80:20 ratio\\nt = flat_array[:split_index]\\nv = flat_array[split_index:]\\nprint(v[1])'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VxQwnZnssq90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Formatting the training and testing data assuming that the training data is put into the variable t, and the testing data is put into the variable v\n",
        "\n",
        "# Normalize the data\n",
        "\n",
        "flat_t=t.flatten()\n",
        "mean=np.mean(flat_t)\n",
        "stdev=np.std(flat_t)\n",
        "flat_t=(flat_t-mean)/stdev\n",
        "t_normalized=flat_t.reshape(t.shape)\n",
        "flat_v=v.flatten()\n",
        "mean=np.mean(flat_v)\n",
        "stdev=np.std(flat_v)\n",
        "flat_v=(flat_v-mean)/stdev\n",
        "v_normalized=flat_v.reshape(v.shape)\n",
        "'''row_sums = t.sum(axis=1)\n",
        "t_normalized = t / row_sums[:, np.newaxis]\n",
        "\n",
        "row_sums_2 = v.sum(axis=1)\n",
        "v_normalized = v / row_sums[:, np.newaxis]'''\n",
        "#t_normalized= t.astype(np.float32) / 255.0\n",
        "#v_normalized = v.astype(np.float32) / 255.0\n",
        "\n",
        "# Create a dummy structure to feed into ImageDataGenerator\n",
        "# This step is more conceptual; in a real-world scenario, you'd likely use directories.\n",
        "train_dummy_labels = np.zeros((t.shape[0],))  # Placeholder labels (assuming binary classification)\n",
        "test_dummy_labels = np.zeros((v.shape[0],))    # Placeholder labels\n",
        "\n",
        "# Create an ImageDataGenerator instance for training\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,          # normalize pixel values to [0,1]\n",
        "    rotation_range=40,       # randomly rotate images in the range (degrees, 0 to 180)\n",
        "    width_shift_range=0.2,   # randomly shift images horizontally (fraction of total width)\n",
        "    height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
        "    shear_range=0.2,         # set range for random shear\n",
        "    zoom_range=0.2,          # set range for random zoom\n",
        "    horizontal_flip=True,    # randomly flip images\n",
        "    fill_mode='nearest'      # fill in newly created pixels\n",
        ")\n",
        "train_generator = train_datagen.flow(t_normalized, train_dummy_labels, batch_size=32)\n",
        "\n",
        "# Create an ImageDataGenerator instance for validation\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)  # Rescale for normalization (as v_normalized is already normalized)\n",
        "test_generator = test_datagen.flow(v_normalized, test_dummy_labels, batch_size=32)\n"
      ],
      "metadata": {
        "id": "xZ8R1dypy3ID",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "42ebd767-87f5-4ab7-fc68-36dc2deede53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-b9ad3b9ab39a>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mflat_t\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mstdev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mflat_t\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_t\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mstdev\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m   3430\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3432\u001b[0;31m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0m\u001b[1;32m   3433\u001b[0m                           out=out, **kwargs)\n\u001b[1;32m   3434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mis_float16_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         ret = um.true_divide(\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (551,980) (5361,6000) "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step two: Actual CNN model\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "\n",
        "# First convolutional layer\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))\n",
        "model.add(MaxPooling2D(2, 2))\n",
        "\n",
        "# Second convolutional layer\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(2, 2))\n",
        "\n",
        "# Third convolutional layer\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(2, 2))\n",
        "\n",
        "# Flatten the output of the convolutional layers\n",
        "model.add(Flatten())\n",
        "\n",
        "# Dense layers for classification\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.5))  # Dropout for regularization\n",
        "model.add(Dense(1, activation='sigmoid'))  # Sigmoid activation for binary classification\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Assuming you have a dataset with train_data, train_labels, test_data, test_labels\n",
        "# train_data and test_data should be in shape (num_samples, 150, 150, 3)\n",
        "# train_labels and test_labels should be binary labels (0 or 1)\n",
        "\n",
        "# Train the model\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=len(t) // 32,  # total_train_samples should be the total number of training samples\n",
        "    epochs=20,\n",
        "    validation_data=test_generator,\n",
        "    validation_steps=len(v) // 32  # total_validation_samples should be the total number of validation samples\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(test_data, test_labels)\n",
        "print(\"Accuracy\"+str(test_acc))\n",
        "\n"
      ],
      "metadata": {
        "id": "QYNKzEXsoVID"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}